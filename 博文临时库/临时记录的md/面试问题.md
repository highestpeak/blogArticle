# 海量数据类型

总的来说要判断文件是否太大不能够装入内存，如果不能装入内存，那么将文件分割成小的文件：通过对记录进行hash处理（对100取模或对1000取模等）以分布到不同小文件，

如果需要求出现次数最多的前n项：可以先求每个文件出现最多的前n项，然后再进一步比较所有文件（采用`hashmap`进行键值对计数）。也可以用n个元素的最小堆（求最少的前n项用最大堆），到达n个元素后比堆顶小就抛弃，比堆顶大就插入。

对不重复的个数可以放入内存的情况：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。(具体情况具体分析)

字符串的最频繁项可以考虑trie树



典型的有限内存的海量数据处理的题目。一般这类题目的解答无非是以下几种：

分治，hash映射，堆排序，双层桶划分，Bloom Filter，bitmap，数据库索引，mapreduce等。

## 读取大文件

- 循环读取定长字节流，存到缓冲区，使用`BufferedReader`等
- 文件管道，使用NIO的channel，
- 内存文件映射
- 将大文件分割成小文件（通过hash处理）

# 秒杀场景

高并发问题，超卖问题，缓存击穿如何解决，

BloomFilter，BloomFilter的底层实现，

容量和容错率。如何说了下前端方面怎么做。

# 线上排查



# 消息队列

生产者消费者问题